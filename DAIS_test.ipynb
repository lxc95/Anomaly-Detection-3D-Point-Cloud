{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SWY0Kp9qzwPq"
   },
   "source": [
    "## Note that:\n",
    "The folder, 'DAIS2023CompetitionSubmission_Liu', includes 2 '.ipynb' files, a 'Datasets' folder, a 'BaselineModel_PointNet' folder and a final report in '.pdf'.\n",
    "\n",
    "You should redirect this file to your working folder by edit the code, \"os.chdir(...)\".\n",
    "\n",
    "Copy the test dataset into your working folder, i.e. '/DAIS2023CompetitionSubmission_Liu/Datasets/'.\n",
    "\n",
    "Then start to run the whole file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Load Testing Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 11081,
     "status": "ok",
     "timestamp": 1678212696738,
     "user": {
      "displayName": "Xinchao Liu",
      "userId": "01601784096942622651"
     },
     "user_tz": 300
    },
    "id": "qxsXWrkx6lPj",
    "outputId": "7650efa3-a990-4005-a678-b44f9411e842"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import proj3d\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "os.chdir('C:/Users/xl037/PycharmProjects/DAIS2023CompetitionSubmission_Liu/Datasets/')\n",
    "cwd = os.getcwd()\n",
    "# print(cwd)\n",
    "test_data_Test_path = os.path.join(cwd, 'test_data.npy')\n",
    "label_data_Test_path = os.path.join(cwd, 'test_label.npy')\n",
    "\n",
    "test_data_Test = np.load(test_data_Test_path)\n",
    "label_data_Test = np.load(label_data_Test_path)\n",
    "\n",
    "print('The number of testing samples is', np.shape(test_data_Test)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cENdnG7dBK5q"
   },
   "source": [
    "# **Load Training Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wfsv3OD37GW0"
   },
   "source": [
    "Here are the plots of all 3D point clouds from 190 samples. Green denotes category '0' and red denotes category '1'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 11081,
     "status": "ok",
     "timestamp": 1678212696738,
     "user": {
      "displayName": "Xinchao Liu",
      "userId": "01601784096942622651"
     },
     "user_tz": 300
    },
    "id": "qxsXWrkx6lPj",
    "outputId": "7650efa3-a990-4005-a678-b44f9411e842"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import proj3d\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "\n",
    "test_data_Train_path = os.path.join(cwd, 'train_data.npy')\n",
    "label_data_Train_path = os.path.join(cwd, 'train_label.npy')\n",
    "\n",
    "test_data = np.load(test_data_Train_path)\n",
    "label_data = np.load(label_data_Train_path)\n",
    "\n",
    "print('The number of training samples is', np.shape(test_data)[0])\n",
    "\n",
    "index_0 = np.where(label_data == 0)[0]\n",
    "index_1 = np.where(label_data == 1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mR6HwS49Bdhy"
   },
   "source": [
    "# **Tensor-Voting based Segmentation for Reference Surface and Anomaly Surface**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cq-htvpDClPA"
   },
   "source": [
    "Before using tensor voting method to calculate all the local features, we can use point cloud denoising method to denoise the whole point cloud data. We have prepared the denoising method, but it is not necessary here since that the current data are not noisy after our pre-checkings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pR12_Yl1yQlA"
   },
   "source": [
    "**Tensor Voting Method** for calculating node-wise physical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 829099,
     "status": "ok",
     "timestamp": 1678213525835,
     "user": {
      "displayName": "Xinchao Liu",
      "userId": "01601784096942622651"
     },
     "user_tz": 300
    },
    "id": "13BgrkOmyXhv",
    "outputId": "0b86f3ce-724c-492b-fb42-e88bb35f4ab4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "!pip install open3d==0.16\n",
    "import open3d as o3d\n",
    "from sklearn.cluster import DBSCAN\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# First, we need to extract the local features of each node using tensor voting method\n",
    "# For each node, find the k-nearest nodes and then calculate the L (Linearness), the P (Planarity), and the S (Sphericity)\n",
    "def LocalFeatures(dataset_temp, num_nodes, radius):\n",
    "  pcd = o3d.geometry.PointCloud()\n",
    "  local_feature_temp = np.zeros((np.shape(dataset_temp)))\n",
    "  pcd.points = o3d.utility.Vector3dVector(dataset_temp)\n",
    "  kdtree = o3d.geometry.KDTreeFlann(pcd)\n",
    "  for i in range(num_nodes):\n",
    "    index_temp = i\n",
    "    radius_temp = radius\n",
    "    k = 0\n",
    "    while k < 100:\n",
    "      radius_temp = radius_temp*1.1\n",
    "      k, idx, _ = kdtree.search_radius_vector_3d(pcd.points[index_temp], radius_temp)\n",
    "      # print(k)\n",
    "    neighbors = dataset_temp[idx, :]\n",
    "    cov_temp = np.zeros((3, 3))\n",
    "    mean = np.mean(neighbors, 0)\n",
    "    for j in range(k):\n",
    "      cov_temp += (neighbors[j, :] - mean.reshape((1, 3))).T @ (neighbors[j, :] - mean.reshape((1, 3))) / k\n",
    "    # the eigen decomposition\n",
    "    Lambda, U = np.linalg.eig(cov_temp)\n",
    "    Lambda = list(Lambda)\n",
    "    Lambda.sort(reverse=True)\n",
    "    # print(Lambda)\n",
    "    local_feature_temp[index_temp, 0] = Lambda[0] - Lambda[1]  # the L (Linearness)\n",
    "    local_feature_temp[index_temp, 1] = Lambda[1] - Lambda[2]  # the P (Planarity)\n",
    "    local_feature_temp[index_temp, 2] = Lambda[2]  # the S (Sphericity)\n",
    "#     print(k)\n",
    "  # print(local_feature_temp)\n",
    "  return local_feature_temp\n",
    "\n",
    "# calculate all the local features of all samples\n",
    "local_features_all = []\n",
    "radius_k = 0.1\n",
    "for index in range(np.shape(test_data)[0]): local_features_all.append(LocalFeatures(test_data[index], np.shape(test_data[index])[0], radius_k))\n",
    "\n",
    "# print(local_features_all[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pR12_Yl1yQlA"
   },
   "source": [
    "For test dataset: ** Tensor Voting Method** for calculating node-wise physical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 829099,
     "status": "ok",
     "timestamp": 1678213525835,
     "user": {
      "displayName": "Xinchao Liu",
      "userId": "01601784096942622651"
     },
     "user_tz": 300
    },
    "id": "13BgrkOmyXhv",
    "outputId": "0b86f3ce-724c-492b-fb42-e88bb35f4ab4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "!pip install open3d==0.16\n",
    "import open3d as o3d\n",
    "from sklearn.cluster import DBSCAN\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# First, we need to extract the local features of each node using tensor voting method\n",
    "# For each node, find the k-nearest nodes and then calculate the L (Linearness), the P (Planarity), and the S (Sphericity)\n",
    "def LocalFeatures(dataset_temp, num_nodes, radius):\n",
    "  pcd = o3d.geometry.PointCloud()\n",
    "  local_feature_temp = np.zeros((np.shape(dataset_temp)))\n",
    "  pcd.points = o3d.utility.Vector3dVector(dataset_temp)\n",
    "  kdtree = o3d.geometry.KDTreeFlann(pcd)\n",
    "  for i in range(num_nodes):\n",
    "    index_temp = i\n",
    "    radius_temp = radius\n",
    "    k = 0\n",
    "    while k < 100:\n",
    "      radius_temp = radius_temp*1.1\n",
    "      k, idx, _ = kdtree.search_radius_vector_3d(pcd.points[index_temp], radius_temp)\n",
    "      # print(k)\n",
    "    neighbors = dataset_temp[idx, :]\n",
    "    cov_temp = np.zeros((3, 3))\n",
    "    mean = np.mean(neighbors, 0)\n",
    "    for j in range(k):\n",
    "      cov_temp += (neighbors[j, :] - mean.reshape((1, 3))).T @ (neighbors[j, :] - mean.reshape((1, 3))) / k\n",
    "    # the eigen decomposition\n",
    "    Lambda, U = np.linalg.eig(cov_temp)\n",
    "    Lambda = list(Lambda)\n",
    "    Lambda.sort(reverse=True)\n",
    "    # print(Lambda)\n",
    "    local_feature_temp[index_temp, 0] = Lambda[0] - Lambda[1]  # the L (Linearness)\n",
    "    local_feature_temp[index_temp, 1] = Lambda[1] - Lambda[2]  # the P (Planarity)\n",
    "    local_feature_temp[index_temp, 2] = Lambda[2]  # the S (Sphericity)\n",
    "#     print(k)\n",
    "  # print(local_feature_temp)\n",
    "  return local_feature_temp\n",
    "\n",
    "# calculate all the local features of all samples\n",
    "local_features_all_Test = []\n",
    "radius_k = 0.1\n",
    "for index in range(np.shape(test_data_Test)[0]): local_features_all_Test.append(LocalFeatures(test_data_Test[index], np.shape(test_data_Test[index])[0], radius_k))\n",
    "\n",
    "# print(local_features_all[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Local Features** (L, P, S) of training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 21392,
     "status": "ok",
     "timestamp": 1678213547223,
     "user": {
      "displayName": "Xinchao Liu",
      "userId": "01601784096942622651"
     },
     "user_tz": 300
    },
    "id": "7bJLfrkPB1b-",
    "outputId": "ddb6088f-b7c2-4151-dd16-0ddf8525bae7"
   },
   "outputs": [],
   "source": [
    "# The local features of all samples. It is noted that it is important to check the sensitivity of the searching parameters for selecting the K-nearest nodes\n",
    "# class 0\n",
    "f = plt.figure(figsize=(20, 4))\n",
    "ax1 = f.add_subplot(131)\n",
    "ax2 = f.add_subplot(132)\n",
    "ax3 = f.add_subplot(133)\n",
    "for index_sample in index_0:\n",
    "    temp_feature = list(local_features_all[index_sample][:, 0:1])\n",
    "    temp_feature.sort(reverse=True)\n",
    "    ax1.plot(np.array(range(2048)), temp_feature, c='green')\n",
    "\n",
    "    temp_feature = list(local_features_all[index_sample][:, 1:2])\n",
    "    temp_feature.sort(reverse=True)\n",
    "    ax2.plot(np.array(range(2048)), temp_feature, c='green')\n",
    "\n",
    "    temp_feature = list(local_features_all[index_sample][:, 2:3])\n",
    "    temp_feature.sort(reverse=True)\n",
    "    ax3.plot(np.array(range(2048)), temp_feature, c='green')\n",
    "\n",
    "# class 1\n",
    "for index_sample in index_1:\n",
    "    temp_feature = list(local_features_all[index_sample][:, 0:1])\n",
    "    temp_feature.sort(reverse=True)\n",
    "    ax1.plot(np.array(range(2048)), temp_feature, c='red')\n",
    "\n",
    "    temp_feature = list(local_features_all[index_sample][:, 1:2])\n",
    "    temp_feature.sort(reverse=True)\n",
    "    ax2.plot(np.array(range(2048)), temp_feature, c='red')\n",
    "\n",
    "    temp_feature = list(local_features_all[index_sample][:, 2:3])\n",
    "    temp_feature.sort(reverse=True)\n",
    "    ax3.plot(np.array(range(2048)), temp_feature, c='red')\n",
    "\n",
    "# Set common labels\n",
    "ax1.set_xlabel('Sorted point index')\n",
    "ax1.set_ylabel('L (Linearness) value')\n",
    "ax1.set_title('The Linearness')\n",
    "# Set common labels\n",
    "ax2.set_xlabel('Sorted point index')\n",
    "ax2.set_ylabel('P (Planarity) value')\n",
    "ax2.set_title('The Planarity')\n",
    "# Set common labels\n",
    "ax3.set_xlabel('Sorted point index')\n",
    "ax3.set_ylabel('S (Sphericity) value')\n",
    "ax3.set_title('The Sphericity')\n",
    "f.suptitle('The local features for all samples (green color denotes class 0 and red color denotes class 1)', y=0.0001, fontsize=15)\n",
    "f.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2E90B1t_WVYx"
   },
   "source": [
    "Discussion:\n",
    "\n",
    "The node-wise physical features (including L, S, and P), which indicate physical information, can help us get physical-interpreted segmentations, such as the reference surface and the anomaly surface. If we directly apply clustering methods (such as k-means and DBSCAN) to the coordinates, the segmentations will lack the physical interpretations, e.g., the reference planes and anomalies. Moreover, the direct clustering of the coordinates is not stable or robust, when the shape or the junction saliency is changed. Based on the node-wise physical features (including L, S, and P), we can incorporate different kinds of clustering methods to achieve physical-interpreted features for classification purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f9BXAgXj4ehA"
   },
   "source": [
    "**Gaussian Mixture Model** for selecting reference surface nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 3705,
     "status": "ok",
     "timestamp": 1678213550923,
     "user": {
      "displayName": "Xinchao Liu",
      "userId": "01601784096942622651"
     },
     "user_tz": 300
    },
    "id": "tqxWQhOa4qDH",
    "outputId": "9b2d2fc7-625d-40ac-d297-b9f18a012d19"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "# !pip install open3d==0.16\n",
    "import open3d as o3d\n",
    "from sklearn import mixture\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import Counter\n",
    "import scipy.optimize as optimize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def perp_error(params, xyz):\n",
    "    a, b, c, d = params\n",
    "    x, y, z = xyz\n",
    "    length = np.sqrt(a**2 + b**2 + c**2)\n",
    "    return (np.abs(a * x + b * y + c * z + d) / length).mean()\n",
    "\n",
    "\n",
    "def SVD(X):\n",
    "    # Find the average of points (centroid) along the columns\n",
    "    C = np.average(X, axis=0)\n",
    "    # Create CX vector (centroid to point) matrix\n",
    "    CX = X - C\n",
    "    # Singular value decomposition\n",
    "    U, S, V = np.linalg.svd(CX)\n",
    "    # The last row of V matrix indicate the eigenvectors of\n",
    "    # smallest eigenvalues (singular values).\n",
    "    N = V[-1]\n",
    "    # Extract a, b, c, d coefficients.\n",
    "    x0, y0, z0 = C\n",
    "    a, b, c = N\n",
    "    d = -(a * x0 + b * y0 + c * z0)\n",
    "    return a, b, c, d\n",
    "\n",
    "# os.chdir('/content/drive/MyDrive/DAISproject/Datasets')\n",
    "# # Read point cloud:\n",
    "# cwd = os.getcwd()\n",
    "# test_data_path = os.path.join(cwd, 'train_data.npy')\n",
    "# label_data_path = os.path.join(cwd, 'train_label.npy')\n",
    "\n",
    "# test_data = np.load(test_data_path)\n",
    "# label_data = np.load(label_data_path)\n",
    "\n",
    "# change the index here to see different samples\n",
    "index = 119\n",
    "\n",
    "temp_data = local_features_all[index][:, :]\n",
    "\n",
    "# Normalisation:\n",
    "scaled_points = StandardScaler().fit_transform(temp_data)\n",
    "# Clustering:\n",
    "model = mixture.GaussianMixture(n_components=2, covariance_type='full')  # cluster into 2 categories\n",
    "clusters = model.fit(scaled_points)\n",
    "\n",
    "# pcd = o3d.io.read_point_cloud('./bun_zipper.ply')\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(test_data[index, :, :])\n",
    "# Get points and transform it to a numpy array:\n",
    "points = np.asarray(pcd.points).copy()\n",
    "\n",
    "# Get labels:\n",
    "labels = model.predict(scaled_points)\n",
    "all_labels = labels\n",
    "# Get the number of colors:\n",
    "n_clusters = len(set(labels))\n",
    "n_noise_ = list(labels).count(-1)\n",
    "print(\"Estimated number of clusters: %d\" % n_clusters)\n",
    "print(set(labels))\n",
    "\n",
    "print(all_labels)\n",
    "\n",
    "# choose the cluster of reference surface\n",
    "num_in_clusters = np.array([len(np.where(all_labels == 0)[0]), len(np.where(all_labels == 1)[0])])\n",
    "print(num_in_clusters)\n",
    "nums = num_in_clusters[1000 <num_in_clusters]  # according to the S plot, the number of nodes of the reference surface is always larger than 1000\n",
    "print(nums)\n",
    "print('The index of the cluster w.r.t. the anomaly surface is:', np.where(num_in_clusters == nums)[0])\n",
    "new_labels = all_labels .reshape(2048, 1)\n",
    "index_surface_cluster = np.where(new_labels == np.where(num_in_clusters == nums)[0])[0]\n",
    "# print(index_surface_cluster)\n",
    "# print(len(index_surface_cluster))\n",
    "\n",
    "x0, y0, z0 = test_data[index, index_surface_cluster, 0], test_data[index, index_surface_cluster, 1], test_data[index, index_surface_cluster, 2]\n",
    "# Solve using SVD method.\n",
    "a, b, c, d = SVD(np.array([x0, y0, z0]).T)\n",
    "if a < 0:\n",
    "  a = -a\n",
    "  b = -b\n",
    "  c = -c\n",
    "  d= -d\n",
    "print('Current index of sample:', index)\n",
    "print(\"SVD  abcd: {:.3f} {:.3f} {:.3f} {:.3f}\".format(a, b, c, d))\n",
    "# Orthogonal mean distance\n",
    "print(\"Perp err: {:.5f}\\n\".format(perp_error((a, b, c, d), (x0, y0, z0))))\n",
    "\n",
    "# divide nodes according to the plane\n",
    "coefs = np.array([a, b, c])\n",
    "dis_vec = (np.dot(test_data[index, :, :], coefs) + d)/(np.sqrt(coefs.dot(coefs)))\n",
    "print(np.min(dis_vec), np.max(dis_vec))\n",
    "fig, ax = plt.subplots(figsize =(10, 7))\n",
    "ax.hist(dis_vec)\n",
    "threshold = 0.1\n",
    "index_anomaly_new = np.where(dis_vec > threshold)[0]\n",
    "index_surface_new = np.where(dis_vec <= threshold)[0]\n",
    "# print(index_anomaly_new)\n",
    "# print(index_surface_new)\n",
    "\n",
    "print('The plot of fitted plane and nodes for the ', index, 'th sample')\n",
    "x = np.linspace(-0.6, 0.6, 100)\n",
    "y = np.linspace(-0.6, 0.6, 100)\n",
    "x, y = np.meshgrid(x, y)\n",
    "eq = -(a * x + b * y + d)/c\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(121, projection='3d')\n",
    "ax.plot_surface(x, y, eq)\n",
    "ax.scatter(test_data[index, :, 0], test_data[index, :, 1], test_data[index, :, 2], c='red')\n",
    "ax.set_xlabel('X', fontsize=16)\n",
    "ax.set_ylabel('Y', fontsize=16)\n",
    "ax.set_zlabel('Z', fontsize=16)\n",
    "ax.view_init(10, 25)\n",
    "ax.set_title('View 1', y=0.0001, fontsize=15)\n",
    "\n",
    "ax = fig.add_subplot(122, projection='3d')\n",
    "ax.plot_surface(x, y, eq)\n",
    "ax.scatter(test_data[index, :, 0], test_data[index, :, 1], test_data[index, :, 2], c='red')\n",
    "ax.set_xlabel('X', fontsize=16)\n",
    "ax.set_ylabel('Y', fontsize=16)\n",
    "ax.set_zlabel('Z', fontsize=16)\n",
    "ax.view_init(-10, -25)\n",
    "ax.set_title('View 2', y=0.0001, fontsize=15)\n",
    "\n",
    "fig.set_size_inches(13, 6)\n",
    "fig.suptitle('The fitted reference surface of the sample #{}'.format(index+1), y=0.0001, fontsize=15)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Mapping the labels classes to a color map:\n",
    "colors = plt.get_cmap(\"tab20\")(all_labels / (n_clusters if n_clusters > 0 else 1))\n",
    "# Update points colors:\n",
    "pcd.colors = o3d.utility.Vector3dVector(colors[:, :3])\n",
    "o3d.visualization.draw_plotly([pcd])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MbzEgRLooAZ7"
   },
   "source": [
    "**Segment all samples**:(run the following codes to apply the avove segmentation to all samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0,
     "output_embedded_package_id": "14zPpn7_OVYAF3Ax_J9CZcsJ6MDSjV1za"
    },
    "executionInfo": {
     "elapsed": 426094,
     "status": "ok",
     "timestamp": 1678213977015,
     "user": {
      "displayName": "Xinchao Liu",
      "userId": "01601784096942622651"
     },
     "user_tz": 300
    },
    "id": "0uwF4DdQoCVX",
    "outputId": "fdda8a88-1f63-4a71-aab3-247909ed4773"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "# !pip install open3d==0.16\n",
    "import open3d as o3d\n",
    "from sklearn import mixture\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import Counter\n",
    "import scipy.optimize as optimize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def perp_error(params, xyz):\n",
    "    a, b, c, d = params\n",
    "    x, y, z = xyz\n",
    "    length = np.sqrt(a**2 + b**2 + c**2)\n",
    "    return (np.abs(a * x + b * y + c * z + d) / length).mean()\n",
    "\n",
    "\n",
    "def SVD(X):\n",
    "    # Find the average of points (centroid) along the columns\n",
    "    C = np.average(X, axis=0)\n",
    "    # Create CX vector (centroid to point) matrix\n",
    "    CX = X - C\n",
    "    # Singular value decomposition\n",
    "    U, S, V = np.linalg.svd(CX)\n",
    "    # The last row of V matrix indicate the eigenvectors of\n",
    "    # smallest eigenvalues (singular values).\n",
    "    N = V[-1]\n",
    "    # Extract a, b, c, d coefficients.\n",
    "    x0, y0, z0 = C\n",
    "    a, b, c = N\n",
    "    d = -(a * x0 + b * y0 + c * z0)\n",
    "    return a, b, c, d\n",
    "\n",
    "# os.chdir('/content/drive/MyDrive/DAISproject/Datasets')\n",
    "# # Read point cloud:\n",
    "# cwd = os.getcwd()\n",
    "# test_data_path = os.path.join(cwd, 'train_data.npy')\n",
    "# label_data_path = os.path.join(cwd, 'train_label.npy')\n",
    "\n",
    "# test_data = np.load(test_data_path)\n",
    "# label_data = np.load(label_data_path)\n",
    "\n",
    "data_seg = []\n",
    "data_seg_surface = []\n",
    "for index in range(np.shape(test_data)[0]):\n",
    "    temp_data = local_features_all[index][:, :]\n",
    "\n",
    "    # Normalisation:\n",
    "    scaled_points = StandardScaler().fit_transform(temp_data)\n",
    "    # Clustering:\n",
    "    model = mixture.GaussianMixture(n_components=2, covariance_type='full')  # cluster into 2 categories\n",
    "    clusters = model.fit(scaled_points)\n",
    "\n",
    "    # pcd = o3d.io.read_point_cloud('./bun_zipper.ply')\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(test_data[index, :, :])\n",
    "    # Get points and transform it to a numpy array:\n",
    "    points = np.asarray(pcd.points).copy()\n",
    "\n",
    "    # Get labels:\n",
    "    labels = model.predict(scaled_points)\n",
    "    all_labels = labels\n",
    "    # Get the number of colors:\n",
    "    n_clusters = len(set(labels))\n",
    "    n_noise_ = list(labels).count(-1)\n",
    "    print(\"Estimated number of clusters: %d\" % n_clusters)\n",
    "    print(set(labels))\n",
    "\n",
    "    print(all_labels)\n",
    "\n",
    "    # choose the cluster of reference surface\n",
    "    num_in_clusters = np.array([len(np.where(all_labels == 0)[0]), len(np.where(all_labels == 1)[0])])\n",
    "    print(num_in_clusters)\n",
    "    nums = num_in_clusters[\n",
    "        1000 < num_in_clusters]  # according to the S plot, the number of nodes of the reference surface is always larger than 1000\n",
    "    print(nums)\n",
    "    print('The index of the cluster w.r.t. the anomaly surface is:', np.where(num_in_clusters == nums)[0])\n",
    "    new_labels = all_labels.reshape(2048, 1)\n",
    "    index_surface_cluster = np.where(new_labels == np.where(num_in_clusters == nums)[0])[0]\n",
    "    # print(index_surface_cluster)\n",
    "    # print(len(index_surface_cluster))\n",
    "\n",
    "    x0, y0, z0 = test_data[index, index_surface_cluster, 0], test_data[index, index_surface_cluster, 1], test_data[\n",
    "        index, index_surface_cluster, 2]\n",
    "    # Solve using SVD method.\n",
    "    a, b, c, d = SVD(np.array([x0, y0, z0]).T)\n",
    "    if a < 0:\n",
    "        a = -a\n",
    "        b = -b\n",
    "        c = -c\n",
    "        d = -d\n",
    "    # print('Current index of sample:', index)\n",
    "    # print(\"SVD  abcd: {:.3f} {:.3f} {:.3f} {:.3f}\".format(a, b, c, d))\n",
    "    # # Orthogonal mean distance\n",
    "    # print(\"Perp err: {:.5f}\\n\".format(perp_error((a, b, c, d), (x0, y0, z0))))\n",
    "\n",
    "    # divide nodes according to the plane\n",
    "    coefs = np.array([a, b, c])\n",
    "    dis_vec = (np.dot(test_data[index, :, :], coefs) + d) / (np.sqrt(coefs.dot(coefs)))\n",
    "    print(np.min(dis_vec), np.max(dis_vec))\n",
    "    # fig, ax = plt.subplots(figsize=(10, 7))\n",
    "    # ax.hist(dis_vec)\n",
    "    threshold = 0.1\n",
    "    index_anomaly_new = np.where(dis_vec > threshold)[0]\n",
    "    index_surface_new = np.where(dis_vec <= threshold)[0]\n",
    "    data_seg.append(test_data[index, index_anomaly_new, :])\n",
    "    data_seg_surface.append(test_data[index, index_surface_new, :])\n",
    "\n",
    "    # Mapping the labels classes to a color map:\n",
    "    colors = plt.get_cmap(\"tab20\")(all_labels / (n_clusters if n_clusters > 0 else 1))\n",
    "    # Update points colors:\n",
    "    pcd.colors = o3d.utility.Vector3dVector(colors[:, :3])\n",
    "    o3d.visualization.draw_plotly([pcd])\n",
    "\n",
    "    print('The plot of fitted plane and nodes for the ', index, 'th sample')\n",
    "    x = np.linspace(-0.6, 0.6, 100)\n",
    "    y = np.linspace(-0.6, 0.6, 100)\n",
    "    x, y = np.meshgrid(x, y)\n",
    "    eq = -(a * x + b * y + d) / c\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(121, projection='3d')\n",
    "    ax.plot_surface(x, y, eq)\n",
    "    ax.scatter(data_seg[index][:, 0], data_seg[index][:, 1], data_seg[index][:, 2], c='green')\n",
    "    ax.scatter(data_seg_surface[index][:, 0], data_seg_surface[index][:, 1], data_seg_surface[index][:, 2], c='red')\n",
    "    ax.set_xlabel('X', fontsize=16)\n",
    "    ax.set_ylabel('Y', fontsize=16)\n",
    "    ax.set_zlabel('Z', fontsize=16)\n",
    "    ax.view_init(0, -140)\n",
    "    ax.set_title('View 1', y=0.0001, fontsize=15)\n",
    "\n",
    "    ax = fig.add_subplot(122, projection='3d')\n",
    "    ax.plot_surface(x, y, eq)\n",
    "    ax.scatter(data_seg[index][:, 0], data_seg[index][:, 1], data_seg[index][:, 2], c='green')\n",
    "    ax.scatter(data_seg_surface[index][:, 0], data_seg_surface[index][:, 1], data_seg_surface[index][:, 2], c='red')\n",
    "    ax.set_xlabel('X', fontsize=16)\n",
    "    ax.set_ylabel('Y', fontsize=16)\n",
    "    ax.set_zlabel('Z', fontsize=16)\n",
    "    ax.view_init(-10, 25)\n",
    "    ax.set_title('View 2', y=0.0001, fontsize=15)\n",
    "\n",
    "    fig.set_size_inches(13, 6)\n",
    "    fig.suptitle('The fitted reference surface of the sample #{}'.format(index + 1), y=0.0001, fontsize=15)\n",
    "    plt.show()\n",
    "\n",
    "# the list 'data_seg' contains the anomaly surface nodes for each samples, but it is noted that the numbers of nodes are not consistent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MbzEgRLooAZ7"
   },
   "source": [
    "For test dataset: **Segment all samples**:(run the following codes to apply the avove segmentation to all samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0,
     "output_embedded_package_id": "14zPpn7_OVYAF3Ax_J9CZcsJ6MDSjV1za"
    },
    "executionInfo": {
     "elapsed": 426094,
     "status": "ok",
     "timestamp": 1678213977015,
     "user": {
      "displayName": "Xinchao Liu",
      "userId": "01601784096942622651"
     },
     "user_tz": 300
    },
    "id": "0uwF4DdQoCVX",
    "outputId": "fdda8a88-1f63-4a71-aab3-247909ed4773"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "# !pip install open3d==0.16\n",
    "import open3d as o3d\n",
    "from sklearn import mixture\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import Counter\n",
    "import scipy.optimize as optimize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def perp_error(params, xyz):\n",
    "    a, b, c, d = params\n",
    "    x, y, z = xyz\n",
    "    length = np.sqrt(a**2 + b**2 + c**2)\n",
    "    return (np.abs(a * x + b * y + c * z + d) / length).mean()\n",
    "\n",
    "\n",
    "def SVD(X):\n",
    "    # Find the average of points (centroid) along the columns\n",
    "    C = np.average(X, axis=0)\n",
    "    # Create CX vector (centroid to point) matrix\n",
    "    CX = X - C\n",
    "    # Singular value decomposition\n",
    "    U, S, V = np.linalg.svd(CX)\n",
    "    # The last row of V matrix indicate the eigenvectors of\n",
    "    # smallest eigenvalues (singular values).\n",
    "    N = V[-1]\n",
    "    # Extract a, b, c, d coefficients.\n",
    "    x0, y0, z0 = C\n",
    "    a, b, c = N\n",
    "    d = -(a * x0 + b * y0 + c * z0)\n",
    "    return a, b, c, d\n",
    "\n",
    "data_seg_Test = []\n",
    "data_seg_surface_Test = []\n",
    "for index in range(np.shape(test_data_Test)[0]):\n",
    "    temp_data = local_features_all_Test[index][:, :]\n",
    "\n",
    "    # Normalisation:\n",
    "    scaled_points = StandardScaler().fit_transform(temp_data)\n",
    "    # Clustering:\n",
    "    model = mixture.GaussianMixture(n_components=2, covariance_type='full')  # cluster into 2 categories\n",
    "    clusters = model.fit(scaled_points)\n",
    "\n",
    "    # pcd = o3d.io.read_point_cloud('./bun_zipper.ply')\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(test_data_Test[index, :, :])\n",
    "    # Get points and transform it to a numpy array:\n",
    "    points = np.asarray(pcd.points).copy()\n",
    "\n",
    "    # Get labels:\n",
    "    labels = model.predict(scaled_points)\n",
    "    all_labels = labels\n",
    "    # Get the number of colors:\n",
    "    n_clusters = len(set(labels))\n",
    "    n_noise_ = list(labels).count(-1)\n",
    "    print(\"Estimated number of clusters: %d\" % n_clusters)\n",
    "    print(set(labels))\n",
    "\n",
    "    print(all_labels)\n",
    "\n",
    "    # choose the cluster of reference surface\n",
    "    num_in_clusters = np.array([len(np.where(all_labels == 0)[0]), len(np.where(all_labels == 1)[0])])\n",
    "    print(num_in_clusters)\n",
    "    nums = num_in_clusters[\n",
    "        1000 < num_in_clusters]  # according to the S plot, the number of nodes of the reference surface is always larger than 1000\n",
    "    print(nums)\n",
    "    print('The index of the cluster w.r.t. the anomaly surface is:', np.where(num_in_clusters == nums)[0])\n",
    "    new_labels = all_labels.reshape(2048, 1)\n",
    "    index_surface_cluster = np.where(new_labels == np.where(num_in_clusters == nums)[0])[0]\n",
    "    # print(index_surface_cluster)\n",
    "    # print(len(index_surface_cluster))\n",
    "\n",
    "    x0, y0, z0 = test_data_Test[index, index_surface_cluster, 0], test_data_Test[index, index_surface_cluster, 1], test_data_Test[\n",
    "        index, index_surface_cluster, 2]\n",
    "    # Solve using SVD method.\n",
    "    a, b, c, d = SVD(np.array([x0, y0, z0]).T)\n",
    "    if a < 0:\n",
    "        a = -a\n",
    "        b = -b\n",
    "        c = -c\n",
    "        d = -d\n",
    "    # print('Current index of sample:', index)\n",
    "    # print(\"SVD  abcd: {:.3f} {:.3f} {:.3f} {:.3f}\".format(a, b, c, d))\n",
    "    # # Orthogonal mean distance\n",
    "    # print(\"Perp err: {:.5f}\\n\".format(perp_error((a, b, c, d), (x0, y0, z0))))\n",
    "\n",
    "    # divide nodes according to the plane\n",
    "    coefs = np.array([a, b, c])\n",
    "    dis_vec = (np.dot(test_data_Test[index, :, :], coefs) + d) / (np.sqrt(coefs.dot(coefs)))\n",
    "    print(np.min(dis_vec), np.max(dis_vec))\n",
    "    # fig, ax = plt.subplots(figsize=(10, 7))\n",
    "    # ax.hist(dis_vec)\n",
    "    threshold = 0.1\n",
    "    index_anomaly_new = np.where(dis_vec > threshold)[0]\n",
    "    index_surface_new = np.where(dis_vec <= threshold)[0]\n",
    "    data_seg_Test.append(test_data_Test[index, index_anomaly_new, :])\n",
    "    data_seg_surface_Test.append(test_data_Test[index, index_surface_new, :])\n",
    "\n",
    "    # Mapping the labels classes to a color map:\n",
    "    colors = plt.get_cmap(\"tab20\")(all_labels / (n_clusters if n_clusters > 0 else 1))\n",
    "    # Update points colors:\n",
    "    pcd.colors = o3d.utility.Vector3dVector(colors[:, :3])\n",
    "    o3d.visualization.draw_plotly([pcd])\n",
    "\n",
    "    print('The plot of fitted plane and nodes for the ', index, 'th sample')\n",
    "    x = np.linspace(-0.6, 0.6, 100)\n",
    "    y = np.linspace(-0.6, 0.6, 100)\n",
    "    x, y = np.meshgrid(x, y)\n",
    "    eq = -(a * x + b * y + d) / c\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(121, projection='3d')\n",
    "    ax.plot_surface(x, y, eq)\n",
    "    ax.scatter(data_seg_Test[index][:, 0], data_seg_Test[index][:, 1], data_seg_Test[index][:, 2], c='green')\n",
    "    ax.scatter(data_seg_surface_Test[index][:, 0], data_seg_surface_Test[index][:, 1], data_seg_surface_Test[index][:, 2], c='red')\n",
    "    ax.set_xlabel('X', fontsize=16)\n",
    "    ax.set_ylabel('Y', fontsize=16)\n",
    "    ax.set_zlabel('Z', fontsize=16)\n",
    "    ax.view_init(0, -140)\n",
    "    ax.set_title('View 1', y=0.0001, fontsize=15)\n",
    "\n",
    "    ax = fig.add_subplot(122, projection='3d')\n",
    "    ax.plot_surface(x, y, eq)\n",
    "    ax.scatter(data_seg_Test[index][:, 0], data_seg_Test[index][:, 1], data_seg_Test[index][:, 2], c='green')\n",
    "    ax.scatter(data_seg_surface_Test[index][:, 0], data_seg_surface_Test[index][:, 1], data_seg_surface_Test[index][:, 2], c='red')\n",
    "    ax.set_xlabel('X', fontsize=16)\n",
    "    ax.set_ylabel('Y', fontsize=16)\n",
    "    ax.set_zlabel('Z', fontsize=16)\n",
    "    ax.view_init(-10, 25)\n",
    "    ax.set_title('View 2', y=0.0001, fontsize=15)\n",
    "\n",
    "    fig.set_size_inches(13, 6)\n",
    "    fig.suptitle('The fitted reference surface of the sample #{}'.format(index + 1), y=0.0001, fontsize=15)\n",
    "    plt.show()\n",
    "\n",
    "# the list 'data_seg' contains the anomaly surface nodes for each samples, but it is noted that the numbers of nodes are not consistent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "piKSGIY6oGXG"
   },
   "source": [
    "Aligh all samples by increasing the number of points: (for Wasserstein Distance and the PointNet model) \n",
    "\n",
    "*   step 1. randomly choose one existing node;\n",
    "*   step 2. find K-nearest neighborhoods;\n",
    "*   step 3. use them to intepolate a new node;\n",
    "*   step 4. repeat 1-3 many times as needed.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24081,
     "status": "ok",
     "timestamp": 1678214001086,
     "user": {
      "displayName": "Xinchao Liu",
      "userId": "01601784096942622651"
     },
     "user_tz": 300
    },
    "id": "DKEyLsQhoIfQ",
    "outputId": "3c49603a-8866-4d4f-d89f-f9490e02d91c"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "# !pip install open3d\n",
    "import open3d as o3d\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import proj3d\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def augmentPoints(dataset_temp, num_new, radius):\n",
    "  pcd = o3d.geometry.PointCloud()\n",
    "  # print(np.shape(dataset_temp))\n",
    "  # dataset_temp = dataset_temp[0, :, :]\n",
    "  new_dataset = dataset_temp\n",
    "  pcd.points = o3d.utility.Vector3dVector(dataset_temp)\n",
    "  kdtree = o3d.geometry.KDTreeFlann(pcd)\n",
    "  for i in range(num_new):\n",
    "    index_temp = random.randint(0, np.shape(dataset_temp)[0]-1)\n",
    "    radius_temp = radius\n",
    "    k = 0\n",
    "    while k < 3:\n",
    "      radius_temp = radius_temp*1.1\n",
    "      k, idx, _ = kdtree.search_radius_vector_3d(pcd.points[index_temp], radius_temp)\n",
    "    neighbors = dataset_temp[idx, :]\n",
    "    mean = np.mean(neighbors, 0)\n",
    "    new_dataset = np.vstack((new_dataset, mean.reshape((1, 3))))\n",
    "    # print(k)\n",
    "  # print(np.shape(new_dataset))\n",
    "  return new_dataset\n",
    "\n",
    "\n",
    "# first, find the maximum number of nodes among all samples\n",
    "num_nodes_all = []\n",
    "for i in range(np.shape(test_data)[0]): num_nodes_all.append(np.shape(np.array(data_seg[i]))[0])\n",
    "# print(num_nodes_all)\n",
    "max_num = max(num_nodes_all)\n",
    "print('The maximum number of nodes in different samples:', max_num)\n",
    "# initial search radius, which is adaptively increased later\n",
    "radius_para = 0.00001\n",
    "new_dataset_all = np.zeros((190, max_num, 3))\n",
    "for j in range(np.shape(test_data)[0]): new_dataset_all[j, :, :]= augmentPoints(np.array(data_seg[j]), max_num-num_nodes_all[j], radius_para)\n",
    "# print(np.shape(new_dataset_all))  # print the shape of anomaly surface nodes\n",
    "\n",
    "# here we save the anomaly surface data from segmentation\n",
    "np.save('new_train_data.npy', new_dataset_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allign the testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24081,
     "status": "ok",
     "timestamp": 1678214001086,
     "user": {
      "displayName": "Xinchao Liu",
      "userId": "01601784096942622651"
     },
     "user_tz": 300
    },
    "id": "DKEyLsQhoIfQ",
    "outputId": "3c49603a-8866-4d4f-d89f-f9490e02d91c"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "# !pip install open3d\n",
    "import open3d as o3d\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import proj3d\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def augmentPoints(dataset_temp, num_new, radius):\n",
    "  pcd = o3d.geometry.PointCloud()\n",
    "  # print(np.shape(dataset_temp))\n",
    "  # dataset_temp = dataset_temp[0, :, :]\n",
    "  new_dataset = dataset_temp\n",
    "  pcd.points = o3d.utility.Vector3dVector(dataset_temp)\n",
    "  kdtree = o3d.geometry.KDTreeFlann(pcd)\n",
    "  for i in range(num_new):\n",
    "    index_temp = random.randint(0, np.shape(dataset_temp)[0]-1)\n",
    "    radius_temp = radius\n",
    "    k = 0\n",
    "    while k < 3:\n",
    "      radius_temp = radius_temp*1.1\n",
    "      k, idx, _ = kdtree.search_radius_vector_3d(pcd.points[index_temp], radius_temp)\n",
    "    neighbors = dataset_temp[idx, :]\n",
    "    mean = np.mean(neighbors, 0)\n",
    "    new_dataset = np.vstack((new_dataset, mean.reshape((1, 3))))\n",
    "    # print(k)\n",
    "  # print(np.shape(new_dataset))\n",
    "  return new_dataset\n",
    "\n",
    "\n",
    "# first, find the maximum number of nodes among all samples\n",
    "num_nodes_all_Test = []\n",
    "for i in range(np.shape(test_data_Test)[0]): num_nodes_all_Test.append(np.shape(np.array(data_seg_Test[i]))[0])\n",
    "# print(num_nodes_all)\n",
    "max_num = max(num_nodes_all)\n",
    "print('The maximum number of nodes in different samples:', max_num)\n",
    "# initial search radius, which is adaptively increased later\n",
    "radius_para = 0.00001\n",
    "new_dataset_all_Test = np.zeros((np.shape(test_data_Test)[0], max_num, 3))\n",
    "for j in range(np.shape(test_data_Test)[0]): new_dataset_all_Test[j, :, :]= augmentPoints(np.array(data_seg_Test[j]), max_num-num_nodes_all_Test[j], radius_para)\n",
    "# print(np.shape(new_dataset_all))  # print the shape of anomaly surface nodes\n",
    "\n",
    "# here we save the anomaly surface data from segmentation\n",
    "np.save('new_train_data_Test.npy', new_dataset_all_Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xwb41xuqoM1R"
   },
   "source": [
    "Plot the anomaly surface example: (for **test dataset**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 6997,
     "status": "ok",
     "timestamp": 1678214008072,
     "user": {
      "displayName": "Xinchao Liu",
      "userId": "01601784096942622651"
     },
     "user_tz": 300
    },
    "id": "ZF_-u7AHoSLY",
    "outputId": "c600f14a-e22e-4f3d-9ce9-4c09e37a3e06"
   },
   "outputs": [],
   "source": [
    "print(np.shape(new_dataset_all_Test))\n",
    "\n",
    "all_index = np.shape(new_dataset_all_Test)[0]\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "for id in range(all_index):\n",
    "  x0 = new_dataset_all_Test[id,:,0]\n",
    "  y0 = new_dataset_all_Test[id,:,1]\n",
    "  z0 = new_dataset_all_Test[id,:,2]\n",
    "  ax.scatter(x0, y0, z0, c='red')\n",
    "# plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "axes3d = fig.add_subplot(111, projection='3d')\n",
    "for id in range(all_index):\n",
    "  x0 = data_seg_Test[id][:,0]\n",
    "  y0 = data_seg_Test[id][:,1]\n",
    "  z0 = data_seg_Test[id][:,2]\n",
    "  axes3d.scatter(x0, y0, z0, c='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bXUQJOxa_wSd"
   },
   "source": [
    "# **Feature Engineering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dctqHV4FAMSd"
   },
   "source": [
    "**Feature 1:** (the vertical distance to the reference surface) N = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 8029,
     "status": "ok",
     "timestamp": 1678214016099,
     "user": {
      "displayName": "Xinchao Liu",
      "userId": "01601784096942622651"
     },
     "user_tz": 300
    },
    "id": "n_DrYV5z_zGK",
    "outputId": "fceea7b8-f6ca-4afe-81a4-5df199fa76a9"
   },
   "outputs": [],
   "source": [
    "# Fit the reference surface (i.e., the plane), using list 'data_seg_surface'\n",
    "import numpy as np\n",
    "import scipy.optimize as optimize\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from mpl_toolkits.mplot3d import proj3d\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# os.chdir('/content/drive/MyDrive/DAISproject/Datasets')\n",
    "# cwd = os.getcwd()\n",
    "# # print(cwd)\n",
    "# test_data_path = os.path.join(cwd, 'train_data.npy')\n",
    "# label_data_path = os.path.join(cwd, 'train_label.npy')\n",
    "\n",
    "# test_data = np.load(test_data_path)\n",
    "# label_data = np.load(label_data_path)\n",
    "\n",
    "\n",
    "def perp_error(params, xyz):\n",
    "    \"\"\"\n",
    "    Mean of the absolute values for the perpendicular distance of the\n",
    "    'xyz' points, to the plane defined by the coefficients 'a,b,c,d' in\n",
    "    'params'.\n",
    "    \"\"\"\n",
    "    a, b, c, d = params\n",
    "    x, y, z = xyz\n",
    "    length = np.sqrt(a**2 + b**2 + c**2)\n",
    "    return (np.abs(a * x + b * y + c * z + d) / length).mean()\n",
    "\n",
    "\n",
    "def SVD(X):\n",
    "    \"\"\"\n",
    "    Singular value decomposition method.\n",
    "    Source: https://gist.github.com/lambdalisue/7201028\n",
    "    \"\"\"\n",
    "    # Find the average of points (centroid) along the columns\n",
    "    C = np.average(X, axis=0)\n",
    "\n",
    "    # Create CX vector (centroid to point) matrix\n",
    "    CX = X - C\n",
    "    # Singular value decomposition\n",
    "    U, S, V = np.linalg.svd(CX)\n",
    "    # The last row of V matrix indicate the eigenvectors of\n",
    "    # smallest eigenvalues (singular values).\n",
    "    N = V[-1]\n",
    "\n",
    "    # Extract a, b, c, d coefficients.\n",
    "    x0, y0, z0 = C\n",
    "    a, b, c = N\n",
    "    d = -(a * x0 + b * y0 + c * z0)\n",
    "\n",
    "    return a, b, c, d\n",
    "\n",
    "# calculate the vertical distances\n",
    "vertical_distance_all = []\n",
    "\n",
    "# mean error of surface fitting\n",
    "error_fit = []\n",
    "\n",
    "for index in range(np.shape(test_data)[0]):\n",
    "  x0, y0, z0 = data_seg_surface[index][:, 0], data_seg_surface[index][:, 1], data_seg_surface[index][:, 2]\n",
    "  # Solve using SVD method.\n",
    "  a, b, c, d = SVD(np.array([x0, y0, z0]).T)\n",
    "  # print('Current index of sample:', index)\n",
    "  # print(\"SVD  abcd: {:.3f} {:.3f} {:.3f} {:.3f}\".format(a, b, c, d))\n",
    "  # # Orthogonal mean distance\n",
    "  # print(\"Perp err: {:.5f}\\n\".format(perp_error((a, b, c, d), (x0, y0, z0))))\n",
    "  error_fit.append(perp_error((a, b, c, d), (x0, y0, z0)))\n",
    "  # calculate the vertical distance\n",
    "  n_d, _ = np.shape(data_seg[index])\n",
    "  vertical_distance = -(a * data_seg[index][:, 0] + b * data_seg[index][:, 1] + d)/c - data_seg[index][:, 2]\n",
    "  vertical_distance_all.append(vertical_distance)\n",
    "\n",
    "print('The plot of fitted plane and nodes for the 190th sample:')\n",
    "# plot the plane with nodes for the 190th sample\n",
    "plt.rcParams[\"figure.figsize\"] = [14.00, 3.50]\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "x = np.linspace(-0.6, 0.6, 100)\n",
    "y = np.linspace(-0.6, 0.6, 100)\n",
    "x, y = np.meshgrid(x, y)\n",
    "eq = -(a * x + b * y + d)/c\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(x, y, eq)\n",
    "ax.scatter(x0, y0, z0, c='red')\n",
    "ax.view_init(30, 45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For test dataset:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 8029,
     "status": "ok",
     "timestamp": 1678214016099,
     "user": {
      "displayName": "Xinchao Liu",
      "userId": "01601784096942622651"
     },
     "user_tz": 300
    },
    "id": "n_DrYV5z_zGK",
    "outputId": "fceea7b8-f6ca-4afe-81a4-5df199fa76a9"
   },
   "outputs": [],
   "source": [
    "# Fit the reference surface (i.e., the plane), using list 'data_seg_surface'\n",
    "import numpy as np\n",
    "import scipy.optimize as optimize\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from mpl_toolkits.mplot3d import proj3d\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "\n",
    "def perp_error(params, xyz):\n",
    "    \"\"\"\n",
    "    Mean of the absolute values for the perpendicular distance of the\n",
    "    'xyz' points, to the plane defined by the coefficients 'a,b,c,d' in\n",
    "    'params'.\n",
    "    \"\"\"\n",
    "    a, b, c, d = params\n",
    "    x, y, z = xyz\n",
    "    length = np.sqrt(a**2 + b**2 + c**2)\n",
    "    return (np.abs(a * x + b * y + c * z + d) / length).mean()\n",
    "\n",
    "\n",
    "def SVD(X):\n",
    "    \"\"\"\n",
    "    Singular value decomposition method.\n",
    "    Source: https://gist.github.com/lambdalisue/7201028\n",
    "    \"\"\"\n",
    "    # Find the average of points (centroid) along the columns\n",
    "    C = np.average(X, axis=0)\n",
    "\n",
    "    # Create CX vector (centroid to point) matrix\n",
    "    CX = X - C\n",
    "    # Singular value decomposition\n",
    "    U, S, V = np.linalg.svd(CX)\n",
    "    # The last row of V matrix indicate the eigenvectors of\n",
    "    # smallest eigenvalues (singular values).\n",
    "    N = V[-1]\n",
    "\n",
    "    # Extract a, b, c, d coefficients.\n",
    "    x0, y0, z0 = C\n",
    "    a, b, c = N\n",
    "    d = -(a * x0 + b * y0 + c * z0)\n",
    "\n",
    "    return a, b, c, d\n",
    "\n",
    "# calculate the vertical distances\n",
    "vertical_distance_all_Test = []\n",
    "\n",
    "# mean error of surface fitting\n",
    "error_fit_Test = []\n",
    "\n",
    "for index in range(np.shape(test_data_Test)[0]):\n",
    "  x0, y0, z0 = data_seg_surface_Test[index][:, 0], data_seg_surface_Test[index][:, 1], data_seg_surface_Test[index][:, 2]\n",
    "  # Solve using SVD method.\n",
    "  a, b, c, d = SVD(np.array([x0, y0, z0]).T)\n",
    "  # print('Current index of sample:', index)\n",
    "  # print(\"SVD  abcd: {:.3f} {:.3f} {:.3f} {:.3f}\".format(a, b, c, d))\n",
    "  # # Orthogonal mean distance\n",
    "  # print(\"Perp err: {:.5f}\\n\".format(perp_error((a, b, c, d), (x0, y0, z0))))\n",
    "  error_fit_Test.append(perp_error((a, b, c, d), (x0, y0, z0)))\n",
    "  # calculate the vertical distance\n",
    "  n_d, _ = np.shape(data_seg_Test[index])\n",
    "  vertical_distance = -(a * data_seg_Test[index][:, 0] + b * data_seg_Test[index][:, 1] + d)/c - data_seg_Test[index][:, 2]\n",
    "  vertical_distance_all_Test.append(vertical_distance)\n",
    "\n",
    "print('The plot of fitted plane and nodes for the last testing sample:')\n",
    "# plot the plane with nodes for the 190th sample\n",
    "plt.rcParams[\"figure.figsize\"] = [14.00, 3.50]\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "x = np.linspace(-0.6, 0.6, 100)\n",
    "y = np.linspace(-0.6, 0.6, 100)\n",
    "x, y = np.meshgrid(x, y)\n",
    "eq = -(a * x + b * y + d)/c\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(x, y, eq)\n",
    "ax.scatter(x0, y0, z0, c='red')\n",
    "ax.view_init(30, 45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "88v4a4FmdrPj"
   },
   "source": [
    "calculate the mean and the quantiles of the distribution of the vertical distance: (quantiles are 0.025, 0.25, 0.50, 0.75, 0.975, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1678214016099,
     "user": {
      "displayName": "Xinchao Liu",
      "userId": "01601784096942622651"
     },
     "user_tz": 300
    },
    "id": "7ZaRAjlEW8dn"
   },
   "outputs": [],
   "source": [
    "# Feature 1\n",
    "# the mean of the vertical distance\n",
    "mean_vertiDistance_all = []\n",
    "# the quantiles of the distribution of the vertical distance\n",
    "quantiles_vertiDistance_all = []\n",
    "for i in range(np.shape(test_data)[0]): mean_vertiDistance_all.append(np.mean(vertical_distance_all[i])/np.shape(vertical_distance_all[i]))\n",
    "for i in range(np.shape(test_data)[0]): quantiles_vertiDistance_all.append([np.quantile(vertical_distance_all[i], 0.025), np.quantile(vertical_distance_all[i], 0.25), np.quantile(vertical_distance_all[i], 0.5), np.quantile(vertical_distance_all[i], 0.75), np.quantile(vertical_distance_all[i], 0.975), np.quantile(vertical_distance_all[i], 1)])\n",
    "# print(mean_vertiDistance_all)\n",
    "# print(quantiles_vertiDistance_all)\n",
    "feature_1_mean = mean_vertiDistance_all\n",
    "feature_1_quantiles = quantiles_vertiDistance_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1678214016099,
     "user": {
      "displayName": "Xinchao Liu",
      "userId": "01601784096942622651"
     },
     "user_tz": 300
    },
    "id": "7ZaRAjlEW8dn"
   },
   "outputs": [],
   "source": [
    "# Feature 1 for testing dataset\n",
    "# the mean of the vertical distance\n",
    "mean_vertiDistance_all_Test = []\n",
    "# the quantiles of the distribution of the vertical distance\n",
    "quantiles_vertiDistance_all_Test = []\n",
    "for i in range(np.shape(test_data_Test)[0]): mean_vertiDistance_all_Test.append(np.mean(vertical_distance_all_Test[i])/np.shape(vertical_distance_all_Test[i]))\n",
    "for i in range(np.shape(test_data_Test)[0]): quantiles_vertiDistance_all_Test.append([np.quantile(vertical_distance_all_Test[i], 0.025), np.quantile(vertical_distance_all_Test[i], 0.25), np.quantile(vertical_distance_all_Test[i], 0.5), np.quantile(vertical_distance_all_Test[i], 0.75), np.quantile(vertical_distance_all_Test[i], 0.975), np.quantile(vertical_distance_all_Test[i], 1)])\n",
    "# print(mean_vertiDistance_all)\n",
    "# print(quantiles_vertiDistance_all)\n",
    "feature_1_mean_Test = mean_vertiDistance_all_Test\n",
    "feature_1_quantiles_Test = quantiles_vertiDistance_all_Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gORIOksAizhn"
   },
   "source": [
    "**Feature 2:** (the mean and the deviation of the coordinates of anomaly surface nodes) N = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1678214016100,
     "user": {
      "displayName": "Xinchao Liu",
      "userId": "01601784096942622651"
     },
     "user_tz": 300
    },
    "id": "Wx480g2fjHn0"
   },
   "outputs": [],
   "source": [
    "# Feature 2\n",
    "# the mean of the coordinates of anomaly surface nodes\n",
    "feature_2_mean = []\n",
    "feature_2_deviation = []\n",
    "for i in range(np.shape(test_data)[0]): feature_2_mean.append([np.mean(data_seg[i][:, 0]), np.mean(data_seg[i][:, 1]), np.mean(data_seg[i][:, 2])])\n",
    "for i in range(np.shape(test_data)[0]): feature_2_deviation.append([np.std(data_seg[i][:, 0]), np.std(data_seg[i][:, 1]), np.std(data_seg[i][:, 2])])\n",
    "# print(feature_2_mean)\n",
    "# print(feature_2_deviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1678214016100,
     "user": {
      "displayName": "Xinchao Liu",
      "userId": "01601784096942622651"
     },
     "user_tz": 300
    },
    "id": "Wx480g2fjHn0"
   },
   "outputs": [],
   "source": [
    "# Feature 2 for testing dataset\n",
    "# the mean of the coordinates of anomaly surface nodes\n",
    "feature_2_mean_Test = []\n",
    "feature_2_deviation_Test = []\n",
    "for i in range(np.shape(test_data_Test)[0]): feature_2_mean_Test.append([np.mean(data_seg_Test[i][:, 0]), np.mean(data_seg_Test[i][:, 1]), np.mean(data_seg_Test[i][:, 2])])\n",
    "for i in range(np.shape(test_data_Test)[0]): feature_2_deviation_Test.append([np.std(data_seg_Test[i][:, 0]), np.std(data_seg_Test[i][:, 1]), np.std(data_seg_Test[i][:, 2])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WGknAHn9mN3X"
   },
   "source": [
    "**Feature 3:** (the size of the dent) N = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1678214016100,
     "user": {
      "displayName": "Xinchao Liu",
      "userId": "01601784096942622651"
     },
     "user_tz": 300
    },
    "id": "TYmXkRQNmVB3"
   },
   "outputs": [],
   "source": [
    "# Feature 3\n",
    "feature_3_length = []\n",
    "feature_3_width = []\n",
    "for i in range(np.shape(test_data)[0]): feature_3_length.append(np.max(data_seg[i][:, 0]) - np.min(data_seg[i][:, 0]))\n",
    "for i in range(np.shape(test_data)[0]): feature_3_width.append(np.max(data_seg[i][:, 1]) - np.min(data_seg[i][:, 1]))\n",
    "# print(feature_3_length)\n",
    "# print(feature_3_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1678214016100,
     "user": {
      "displayName": "Xinchao Liu",
      "userId": "01601784096942622651"
     },
     "user_tz": 300
    },
    "id": "TYmXkRQNmVB3"
   },
   "outputs": [],
   "source": [
    "# Feature 3\n",
    "feature_3_length_Test = []\n",
    "feature_3_width_Test = []\n",
    "for i in range(np.shape(test_data_Test)[0]): feature_3_length_Test.append(np.max(data_seg_Test[i][:, 0]) - np.min(data_seg_Test[i][:, 0]))\n",
    "for i in range(np.shape(test_data_Test)[0]): feature_3_width_Test.append(np.max(data_seg_Test[i][:, 1]) - np.min(data_seg_Test[i][:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2wfvBMuoUGH"
   },
   "source": [
    "**Feature 4:** (the number of the anomaly surface nodes) N = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1678214016100,
     "user": {
      "displayName": "Xinchao Liu",
      "userId": "01601784096942622651"
     },
     "user_tz": 300
    },
    "id": "M8YKwifpobkN"
   },
   "outputs": [],
   "source": [
    "# Feature 4\n",
    "feature_4 = []\n",
    "for i in range(np.shape(test_data)[0]): feature_4.append(np.shape(data_seg[i])[0])\n",
    "# print(feature_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1678214016100,
     "user": {
      "displayName": "Xinchao Liu",
      "userId": "01601784096942622651"
     },
     "user_tz": 300
    },
    "id": "M8YKwifpobkN"
   },
   "outputs": [],
   "source": [
    "# Feature 4\n",
    "feature_4_Test = []\n",
    "for i in range(np.shape(test_data_Test)[0]): feature_4_Test.append(np.shape(data_seg_Test[i])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dm-_eNO2qbCR"
   },
   "source": [
    "**Feature 5:** (the mean and the deviation of the coordinates of reference surface nodes) N = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1678214016100,
     "user": {
      "displayName": "Xinchao Liu",
      "userId": "01601784096942622651"
     },
     "user_tz": 300
    },
    "id": "vcNeF_LBqm3E"
   },
   "outputs": [],
   "source": [
    "# Feature 5\n",
    "# the mean of the coordinates of anomaly surface nodes\n",
    "feature_5_mean = []\n",
    "feature_5_deviation = []\n",
    "for i in range(np.shape(test_data)[0]): feature_5_mean.append([np.mean(data_seg_surface[i][:, 0]), np.mean(data_seg_surface[i][:, 1]), np.mean(data_seg_surface[i][:, 2])])\n",
    "for i in range(np.shape(test_data)[0]): feature_5_deviation.append([np.std(data_seg_surface[i][:, 0]), np.std(data_seg_surface[i][:, 1]), np.std(data_seg_surface[i][:, 2])])\n",
    "# print(feature_5_mean)\n",
    "# print(feature_5_deviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1678214016100,
     "user": {
      "displayName": "Xinchao Liu",
      "userId": "01601784096942622651"
     },
     "user_tz": 300
    },
    "id": "vcNeF_LBqm3E"
   },
   "outputs": [],
   "source": [
    "# Feature 5\n",
    "# the mean of the coordinates of anomaly surface nodes\n",
    "feature_5_mean_Test = []\n",
    "feature_5_deviation_Test = []\n",
    "for i in range(np.shape(test_data_Test)[0]): feature_5_mean_Test.append([np.mean(data_seg_surface_Test[i][:, 0]), np.mean(data_seg_surface_Test[i][:, 1]), np.mean(data_seg_surface_Test[i][:, 2])])\n",
    "for i in range(np.shape(test_data_Test)[0]): feature_5_deviation_Test.append([np.std(data_seg_surface_Test[i][:, 0]), np.std(data_seg_surface_Test[i][:, 1]), np.std(data_seg_surface_Test[i][:, 2])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x0IKJGBIEfRH"
   },
   "source": [
    "**Feature 6:** (the mean distance of nodes around surface (to the fitted surface) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1678214016100,
     "user": {
      "displayName": "Xinchao Liu",
      "userId": "01601784096942622651"
     },
     "user_tz": 300
    },
    "id": "MegQFff8Ffg9"
   },
   "outputs": [],
   "source": [
    "# Feature 6\n",
    "feature_6_fitErr = []\n",
    "for i in range(np.shape(test_data)[0]): feature_6_fitErr.append(error_fit[i])\n",
    "# print(feature_6_fitErr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1678214016100,
     "user": {
      "displayName": "Xinchao Liu",
      "userId": "01601784096942622651"
     },
     "user_tz": 300
    },
    "id": "MegQFff8Ffg9"
   },
   "outputs": [],
   "source": [
    "# Feature 6\n",
    "feature_6_fitErr_Test = []\n",
    "for i in range(np.shape(test_data_Test)[0]): feature_6_fitErr_Test.append(error_fit_Test[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gmOf23HoILRt"
   },
   "source": [
    "**Feature 7:** (the Hausdorff Distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4195,
     "status": "ok",
     "timestamp": 1678214020291,
     "user": {
      "displayName": "Xinchao Liu",
      "userId": "01601784096942622651"
     },
     "user_tz": 300
    },
    "id": "07qbP-IerHug",
    "outputId": "e795b444-4b6e-40d5-c2a5-6c16f161a19e"
   },
   "outputs": [],
   "source": [
    "!pip install point-cloud-utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 64549,
     "status": "ok",
     "timestamp": 1678214084837,
     "user": {
      "displayName": "Xinchao Liu",
      "userId": "01601784096942622651"
     },
     "user_tz": 300
    },
    "id": "zrWb9MjCIWxS",
    "outputId": "49da854d-434b-4878-cb62-547e05e39e23"
   },
   "outputs": [],
   "source": [
    "# Feature 7\n",
    "import point_cloud_utils as pcu\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import scipy.sparse as sparse\n",
    "import os\n",
    "\n",
    "\n",
    "index_0 = np.where(label_data == 0)[0]\n",
    "index_1 = np.where(label_data == 1)[0]\n",
    "\n",
    "# Hausdorff distance\n",
    "hausdorff_dist_matrix = np.zeros((190, 190))\n",
    "for i in list(index_0) + list(index_1):\n",
    "  for j in list(index_0) + list(index_1):\n",
    "    dataset_class0 = data_seg[i]\n",
    "    dataset_class1 = data_seg[j]\n",
    "    # Take a max of the one sided squared  distances to get the two sided Hausdorff distance\n",
    "    hausdorff_dist_matrix[i, j] = pcu.hausdorff_distance(dataset_class0, dataset_class1)\n",
    "# visualize the sparse matrix with Spy\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.matshow(hausdorff_dist_matrix)\n",
    "\n",
    "# Chamfer distance\n",
    "chamfer_dist_matrix = chamfer_dist_matrix = np.zeros((190, 190))\n",
    "for i in list(index_0) + list(index_1):\n",
    "  for j in list(index_0) + list(index_1):\n",
    "    dataset_class0 = data_seg[i]\n",
    "    dataset_class1 = data_seg[j]\n",
    "    # Take a max of the one sided squared  distances to get the two sided Hausdorff distance\n",
    "    # chamfer_dist_matrix[i, j] = pcu.hausdorff_distance(dataset_class0, dataset_class1)\n",
    "    chamfer_dist_matrix[i, j] = pcu.chamfer_distance(dataset_class0, dataset_class1)\n",
    "# visualize the sparse matrix with Spy\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.matshow(chamfer_dist_matrix)\n",
    "\n",
    "# # print(np.shape(hausdorff_dist_matrix))\n",
    "\n",
    "# # # Approximate Wasserstein (Sinkhorn) distance\n",
    "# # Wasserstein_dist_matrix = np.zeros((190, 190))\n",
    "# # for i in list(index_0) + list(index_1):\n",
    "# #   for j in list(index_0) + list(index_1):\n",
    "# #     dataset_class0 = data_seg[i]\n",
    "# #     dataset_class1 = data_seg[j]\n",
    "# #     # a and b are arrays where each row contains a point\n",
    "# #     # Note that the point sets can have different sizes (e.g [100, 3], [111, 3])\n",
    "# #     a = dataset_class0\n",
    "# #     b = dataset_class1\n",
    "\n",
    "# #     # M is a 100x100 array where each entry  (i, j) is the L2 distance between point a[i, :] and b[j, :]\n",
    "# #     M = pcu.pairwise_distances(a, b)\n",
    "# #     M = M.astype('float64')\n",
    "\n",
    "# #     # w_a and w_b are masses assigned to each point. In this case each point is weighted equally.\n",
    "# #     w_a = np.ones(a.shape[0])\n",
    "# #     w_b = np.ones(b.shape[0])\n",
    "\n",
    "# #     # P is the transport matrix between a and b, eps is a regularization parameter, smaller epsilons lead to\n",
    "# #     # better approximation of the true Wasserstein distance at the expense of slower convergence\n",
    "# #     P = pcu.sinkhorn(w_a, w_b, M, eps=1e-3)\n",
    "\n",
    "# #     # To get the distance as a number just compute the frobenius inner product <M, P>\n",
    "# #     sinkhorn_dist = (M*P).sum()\n",
    "# #     Wasserstein_dist_matrix[i, j] = sinkhorn_dist\n",
    "# # # visualize the sparse matrix with Spy\n",
    "# # fig = plt.figure(figsize=(10, 10))\n",
    "# # ax = fig.add_subplot(111)\n",
    "# # ax.matshow(Wasserstein_dist_matrix)\n",
    "\n",
    "# # np.save('Wasserstein_distance.npy', hausdorff_dist_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For test dataset:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 64549,
     "status": "ok",
     "timestamp": 1678214084837,
     "user": {
      "displayName": "Xinchao Liu",
      "userId": "01601784096942622651"
     },
     "user_tz": 300
    },
    "id": "zrWb9MjCIWxS",
    "outputId": "49da854d-434b-4878-cb62-547e05e39e23"
   },
   "outputs": [],
   "source": [
    "# Feature 7 for test dataset\n",
    "import point_cloud_utils as pcu\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import scipy.sparse as sparse\n",
    "import os\n",
    "\n",
    "\n",
    "index_0 = np.where(label_data == 0)[0]\n",
    "index_1 = np.where(label_data == 1)[0]\n",
    "\n",
    "num_Test = np.shape(test_data_Test)[0]\n",
    "\n",
    "# Hausdorff distance\n",
    "hausdorff_dist_matrix_Test = np.zeros((190, num_Test))\n",
    "for i in list(index_0) + list(index_1):\n",
    "  for j in range(num_Test):\n",
    "    dataset_class0 = data_seg[i]\n",
    "    dataset_class1 = data_seg_Test[j]\n",
    "    # Take a max of the one sided squared  distances to get the two sided Hausdorff distance\n",
    "    hausdorff_dist_matrix_Test[i, j] = pcu.hausdorff_distance(dataset_class0, dataset_class1)\n",
    "# visualize the sparse matrix with Spy\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.matshow(hausdorff_dist_matrix_Test)\n",
    "\n",
    "# Chamfer distance\n",
    "chamfer_dist_matrix_Test = np.zeros((190, num_Test))\n",
    "for i in list(index_0) + list(index_1):\n",
    "  for j in range(num_Test):\n",
    "    dataset_class0 = data_seg[i]\n",
    "    dataset_class1 = data_seg_Test[j]\n",
    "    # Take a max of the one sided squared  distances to get the two sided Hausdorff distance\n",
    "    # chamfer_dist_matrix[i, j] = pcu.hausdorff_distance(dataset_class0, dataset_class1)\n",
    "    chamfer_dist_matrix_Test[i, j] = pcu.chamfer_distance(dataset_class0, dataset_class1)\n",
    "# visualize the sparse matrix with Spy\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.matshow(chamfer_dist_matrix_Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fkWzlBuCqnf8"
   },
   "source": [
    "Using augmented data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1678214084837,
     "user": {
      "displayName": "Xinchao Liu",
      "userId": "01601784096942622651"
     },
     "user_tz": 300
    },
    "id": "myKc9DSGqqur"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import numpy as np\n",
    "# BASE_DIR = os.path.dirname(\"C:/Users/xl037/PycharmProjects/DAISproject/\")\n",
    "# DATA_DIR = os.path.join(BASE_DIR, 'Datasets')\n",
    "# train_data_path = os.path.join(DATA_DIR, 'new_train_data.npy')\n",
    "# new_data = np.load(train_data_path)\n",
    "\n",
    "# # Approximate Wasserstein (Sinkhorn) distance\n",
    "# Wasserstein_dist_matrix = np.zeros((190, 190))\n",
    "# num_ = 0\n",
    "# for i in list(index_0) + list(index_1):\n",
    "#   print(num_)\n",
    "#   num_ += 1\n",
    "#   for j in list(index_0) + list(index_1):\n",
    "#     dataset_class0 = new_data[i, :, :]\n",
    "#     dataset_class1 = new_data[j, :, :]\n",
    "#     # a and b are arrays where each row contains a point\n",
    "#     # Note that the point sets can have different sizes (e.g [100, 3], [111, 3])\n",
    "#     a = dataset_class0\n",
    "#     b = dataset_class1\n",
    "\n",
    "#     # M is a 100x100 array where each entry  (i, j) is the L2 distance between point a[i, :] and b[j, :]\n",
    "#     M = pcu.pairwise_distances(a, b)\n",
    "#     M = M.astype('float64')\n",
    "\n",
    "#     # w_a and w_b are masses assigned to each point. In this case each point is weighted equally.\n",
    "#     w_a = np.ones(a.shape[0])\n",
    "#     w_b = np.ones(b.shape[0])\n",
    "\n",
    "#     # P is the transport matrix between a and b, eps is a regularization parameter, smaller epsilons lead to\n",
    "#     # better approximation of the true Wasserstein distance at the expense of slower convergence\n",
    "#     P = pcu.sinkhorn(w_a, w_b, M, eps=1e-3)\n",
    "\n",
    "#     # To get the distance as a number just compute the frobenius inner product <M, P>\n",
    "#     sinkhorn_dist = (M*P).sum()\n",
    "#     Wasserstein_dist_matrix[i, j] = sinkhorn_dist\n",
    "# # visualize the sparse matrix with Spy\n",
    "# fig = plt.figure(figsize=(10, 10))\n",
    "# ax = fig.add_subplot(111)\n",
    "# ax.matshow(Wasserstein_dist_matrix)\n",
    "# np.save('aug_Wasserstein_distance.npy', Wasserstein_dist_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**for test dataset:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1678214084837,
     "user": {
      "displayName": "Xinchao Liu",
      "userId": "01601784096942622651"
     },
     "user_tz": 300
    },
    "id": "myKc9DSGqqur"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "cwd = os.getcwd()\n",
    "train_data_path = os.path.join(cwd, 'new_train_data.npy')\n",
    "train_data_Test_path = os.path.join(cwd, 'new_train_data_Test.npy')\n",
    "new_data = np.load(train_data_path)\n",
    "new_data_Test = np.load(train_data_Test_path)\n",
    "\n",
    "num_Test = np.shape(test_data_Test)[0]\n",
    "\n",
    "# Approximate Wasserstein (Sinkhorn) distance\n",
    "Wasserstein_dist_matrix_Test = np.zeros((190, num_Test))\n",
    "num_ = 0\n",
    "for i in list(index_0) + list(index_1):\n",
    "  print(num_)\n",
    "  num_ += 1\n",
    "  for j in range(num_Test):\n",
    "    dataset_class0 = new_data[i, :, :]\n",
    "    dataset_class1 = new_data_Test[j, :, :]\n",
    "    # a and b are arrays where each row contains a point\n",
    "    # Note that the point sets can have different sizes (e.g [100, 3], [111, 3])\n",
    "    a = dataset_class0\n",
    "    b = dataset_class1\n",
    "\n",
    "    # M is a 100x100 array where each entry  (i, j) is the L2 distance between point a[i, :] and b[j, :]\n",
    "    M = pcu.pairwise_distances(a, b)\n",
    "    M = M.astype('float64')\n",
    "\n",
    "    # w_a and w_b are masses assigned to each point. In this case each point is weighted equally.\n",
    "    w_a = np.ones(a.shape[0])\n",
    "    w_b = np.ones(b.shape[0])\n",
    "\n",
    "    # P is the transport matrix between a and b, eps is a regularization parameter, smaller epsilons lead to\n",
    "    # better approximation of the true Wasserstein distance at the expense of slower convergence\n",
    "    P = pcu.sinkhorn(w_a, w_b, M, eps=1e-3)\n",
    "\n",
    "    # To get the distance as a number just compute the frobenius inner product <M, P>\n",
    "    sinkhorn_dist = (M*P).sum()\n",
    "    Wasserstein_dist_matrix_Test[i, j] = sinkhorn_dist\n",
    "# visualize the sparse matrix with Spy\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.matshow(Wasserstein_dist_matrix_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('aug_Wasserstein_distance_Test.npy', Wasserstein_dist_matrix_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 1745,
     "status": "ok",
     "timestamp": 1678214086773,
     "user": {
      "displayName": "Xinchao Liu",
      "userId": "01601784096942622651"
     },
     "user_tz": 300
    },
    "id": "OW0ZzWet0Nvb",
    "outputId": "0a07a1ab-ed80-43dd-9955-ed43f95f5c52"
   },
   "outputs": [],
   "source": [
    "# os.chdir('/content/drive/MyDrive/DAISproject/Datasets')\n",
    "# print(cwd)\n",
    "test_data_path = os.path.join(cwd, 'aug_Wasserstein_distance.npy')\n",
    "Wasserstein_dist_matrix = np.load(test_data_path)\n",
    "# visualize the sparse matrix with Spy\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.matshow(Wasserstein_dist_matrix)\n",
    "\n",
    "print(np.shape(Wasserstein_dist_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DVYIu8AMo34A"
   },
   "source": [
    "**Combine all the features:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1678214086773,
     "user": {
      "displayName": "Xinchao Liu",
      "userId": "01601784096942622651"
     },
     "user_tz": 300
    },
    "id": "H27sWnIU8g5Z",
    "outputId": "257478fd-2910-4afa-f252-ed19863d7548"
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "# define the array for saving all features\n",
    "num_features = 23 + 4*3\n",
    "feature_all = np.zeros((190, num_features))\n",
    "Num_samples = 190\n",
    "# feature 1\n",
    "for i in range(Num_samples): feature_all[i, 0:1] = feature_1_mean[i]\n",
    "for i in range(Num_samples): feature_all[i, 1:7] = feature_1_quantiles[i]\n",
    "# feature 2\n",
    "for i in range(Num_samples): feature_all[i, 7:10] = feature_2_mean[i]\n",
    "for i in range(Num_samples): feature_all[i, 10:13] = feature_2_deviation[i]\n",
    "# feature 3\n",
    "for i in range(Num_samples): feature_all[i, 13:14] = feature_3_length[i]\n",
    "for i in range(Num_samples): feature_all[i, 14:15] = feature_3_width[i]\n",
    "# feature 4\n",
    "for i in range(Num_samples): feature_all[i, 15:16] = feature_4[i]\n",
    "# feature 5\n",
    "for i in range(Num_samples): feature_all[i, 16:19] = feature_5_mean[i]\n",
    "for i in range(Num_samples): feature_all[i, 19:22] = feature_5_deviation[i]\n",
    "#feature 6\n",
    "for i in range(Num_samples): feature_all[i, 22:23] = feature_6_fitErr[i]\n",
    "# print(feature_all)\n",
    "print(np.shape(test_data)[0])\n",
    "# normalize all the data\n",
    "for i in range(num_features):\n",
    "  feature_all[:, i] = preprocessing.normalize([feature_all[:, i]])\n",
    "# print(feature_all)\n",
    "# print(feature_all[:, 36:37])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**for test dataset:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1678214086773,
     "user": {
      "displayName": "Xinchao Liu",
      "userId": "01601784096942622651"
     },
     "user_tz": 300
    },
    "id": "H27sWnIU8g5Z",
    "outputId": "257478fd-2910-4afa-f252-ed19863d7548"
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "# define the array for saving all features\n",
    "num_features = 23 + 4*3\n",
    "feature_all_Test = np.zeros((np.shape(test_data_Test)[0], num_features))\n",
    "Num_samples = np.shape(test_data_Test)[0]\n",
    "# feature 1\n",
    "for i in range(Num_samples): feature_all_Test[i, 0:1] = feature_1_mean_Test[i]\n",
    "for i in range(Num_samples): feature_all_Test[i, 1:7] = feature_1_quantiles_Test[i]\n",
    "# feature 2\n",
    "for i in range(Num_samples): feature_all_Test[i, 7:10] = feature_2_mean_Test[i]\n",
    "for i in range(Num_samples): feature_all_Test[i, 10:13] = feature_2_deviation_Test[i]\n",
    "# feature 3\n",
    "for i in range(Num_samples): feature_all_Test[i, 13:14] = feature_3_length_Test[i]\n",
    "for i in range(Num_samples): feature_all_Test[i, 14:15] = feature_3_width_Test[i]\n",
    "# feature 4\n",
    "for i in range(Num_samples): feature_all_Test[i, 15:16] = feature_4_Test[i]\n",
    "# feature 5\n",
    "for i in range(Num_samples): feature_all_Test[i, 16:19] = feature_5_mean_Test[i]\n",
    "for i in range(Num_samples): feature_all_Test[i, 19:22] = feature_5_deviation_Test[i]\n",
    "#feature 6\n",
    "for i in range(Num_samples): feature_all_Test[i, 22:23] = feature_6_fitErr_Test[i]\n",
    "# print(feature_all)\n",
    "print(np.shape(test_data_Test)[0])\n",
    "# normalize all the data\n",
    "for i in range(num_features):\n",
    "  feature_all_Test[:, i] = preprocessing.normalize([feature_all_Test[:, i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 14692,
     "status": "ok",
     "timestamp": 1678214101460,
     "user": {
      "displayName": "Xinchao Liu",
      "userId": "01601784096942622651"
     },
     "user_tz": 300
    },
    "id": "TfAWPyNt62iU",
    "outputId": "6dbf3dbf-8390-4bdb-f99d-8cdecc26805a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# correlation without advanced distance metrics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def correlation_heatmap(train):\n",
    "    correlations = np.corrcoef(train)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(50,50))\n",
    "    sns.heatmap(correlations, vmax=1.0, center=0, fmt='.2f',\n",
    "                square=True, linewidths=.5, annot=True, annot_kws={\"fontsize\":21}, cbar_kws={\"shrink\": .70})\n",
    "    plt.show()\n",
    "\n",
    "train = np.vstack((feature_all[:, 0:23].reshape(23, 190), label_data.reshape(1, 190)))\n",
    "# train = np.vstack((feature_all[:, 35:36].reshape(1, 190), label_data.reshape(1, 190)))\n",
    "correlation_heatmap(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "frKHdEmyaWRA"
   },
   "source": [
    "# **Balancing, Training, and Classification Prediction (XGboost):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9214,
     "status": "ok",
     "timestamp": 1678214110671,
     "user": {
      "displayName": "Xinchao Liu",
      "userId": "01601784096942622651"
     },
     "user_tz": 300
    },
    "id": "0knGaDP3Wtcg",
    "outputId": "8c3736a6-6d18-4612-dab8-584b6c8c8dc9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install imbalanced-learn\n",
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat the training and prediction process many times\n",
    "# use XGboost\n",
    "# without flip the 0 and 1\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "import point_cloud_utils as pcu\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import scipy.sparse as sparse\n",
    "import random\n",
    "import statistics\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "all_data = np.load(test_data_Train_path)\n",
    "all_label = np.load(label_data_Train_path)\n",
    "all_data_Test = np.load(test_data_Test_path)\n",
    "all_label_Test = np.load(label_data_Test_path)\n",
    "\n",
    "mean_accuracy = []\n",
    "mean_precision = []\n",
    "mean_recall = []\n",
    "mean_F1 = []\n",
    "\n",
    "test_index = list(range(np.shape(test_data_Test)[0]))\n",
    "train_index = list(range(190))\n",
    "\n",
    "train_data = all_data\n",
    "train_label = all_label\n",
    "\n",
    "index_train_0 = np.where(train_label == 0)[0]\n",
    "index_train_1 = np.where(train_label == 1)[0]\n",
    "\"\"\"\n",
    "  feature 7, Hausdorff distance\n",
    "\"\"\"\n",
    "feature_7_hausdorff_dist_all_0 = np.zeros((190, 2))\n",
    "feature_7_hausdorff_dist_all_1 = np.zeros((190, 2))\n",
    "feature_7_hausdorff_dist_all_0_Test = np.zeros((np.shape(test_data_Test)[0], 2))\n",
    "feature_7_hausdorff_dist_all_1_Test = np.zeros((np.shape(test_data_Test)[0], 2))\n",
    "# train\n",
    "for mk in range(190):\n",
    "    temp_0 = []\n",
    "    temp_1 = []\n",
    "    for jk in range(190):\n",
    "        if jk == mk:\n",
    "            continue\n",
    "        else:\n",
    "            if jk in index_train_0:\n",
    "                temp_0.append(hausdorff_dist_matrix[train_index[jk], train_index[mk]])\n",
    "            elif jk in index_train_1:\n",
    "                temp_1.append(hausdorff_dist_matrix[train_index[jk], train_index[mk]])\n",
    "    feature_7_hausdorff_dist_all_0[train_index[mk], 0] = sum(temp_0) / (np.shape(temp_0)[0])\n",
    "    feature_7_hausdorff_dist_all_1[train_index[mk], 0] = sum(temp_1) / (np.shape(temp_1)[0])\n",
    "    feature_7_hausdorff_dist_all_0[train_index[mk], 1] = np.std(temp_0) - np.std(temp_1)\n",
    "    feature_7_hausdorff_dist_all_1[train_index[mk], 1] = np.std(temp_1)\n",
    "# test\n",
    "for mk in range(np.shape(test_data_Test)[0]):\n",
    "    temp_0 = []\n",
    "    temp_1 = []\n",
    "    for jk in range(190):\n",
    "        if jk in index_train_0:\n",
    "            temp_0.append(hausdorff_dist_matrix_Test[train_index[jk], test_index[mk]])\n",
    "        elif jk in index_train_1:\n",
    "            temp_1.append(hausdorff_dist_matrix_Test[train_index[jk], test_index[mk]])\n",
    "    feature_7_hausdorff_dist_all_0_Test[test_index[mk], 0] = sum(temp_0) / (np.shape(temp_0)[0])\n",
    "    feature_7_hausdorff_dist_all_1_Test[test_index[mk], 0] = sum(temp_1) / (np.shape(temp_1)[0])\n",
    "    feature_7_hausdorff_dist_all_0_Test[test_index[mk], 1] = np.std(temp_0) - np.std(temp_1)\n",
    "    feature_7_hausdorff_dist_all_1_Test[test_index[mk], 1] = np.std(temp_1)\n",
    "\"\"\"\n",
    "  feature 7, Chamfer distance\n",
    "\"\"\"\n",
    "feature_7_chamfer_dist_all_0 = np.zeros((190, 2))\n",
    "feature_7_chamfer_dist_all_1 = np.zeros((190, 2))\n",
    "feature_7_chamfer_dist_all_0_Test = np.zeros((np.shape(test_data_Test)[0], 2))\n",
    "feature_7_chamfer_dist_all_1_Test = np.zeros((np.shape(test_data_Test)[0], 2))\n",
    "# train\n",
    "for mk in range(190):\n",
    "    temp_0 = []\n",
    "    temp_1 = []\n",
    "    for jk in range(190):\n",
    "        if jk == mk:\n",
    "            continue\n",
    "        else:\n",
    "            if jk in index_train_0:\n",
    "                temp_0.append(chamfer_dist_matrix[train_index[jk], train_index[mk]])\n",
    "            elif jk in index_train_1:\n",
    "                temp_1.append(chamfer_dist_matrix[train_index[jk], train_index[mk]])\n",
    "    feature_7_chamfer_dist_all_0[train_index[mk], 0] = sum(temp_0) / (np.shape(temp_0)[0])\n",
    "    feature_7_chamfer_dist_all_1[train_index[mk], 0] = sum(temp_1) / (np.shape(temp_1)[0])\n",
    "    feature_7_chamfer_dist_all_0[train_index[mk], 1] = np.std(temp_0) - np.std(temp_1)\n",
    "    feature_7_chamfer_dist_all_1[train_index[mk], 1] = np.std(temp_1)\n",
    "# test\n",
    "for mk in range(np.shape(test_data_Test)[0]):\n",
    "    temp_0 = []\n",
    "    temp_1 = []\n",
    "    for jk in range(190):\n",
    "        if jk in index_train_0:\n",
    "            temp_0.append(chamfer_dist_matrix_Test[train_index[jk], test_index[mk]])\n",
    "        elif jk in index_train_1:\n",
    "            temp_1.append(chamfer_dist_matrix_Test[train_index[jk], test_index[mk]])\n",
    "    feature_7_chamfer_dist_all_0_Test[test_index[mk], 0] = sum(temp_0) / (np.shape(temp_0)[0])\n",
    "    feature_7_chamfer_dist_all_1_Test[test_index[mk], 0] = sum(temp_1) / (np.shape(temp_1)[0])\n",
    "    feature_7_chamfer_dist_all_0_Test[test_index[mk], 1] = np.std(temp_0) - np.std(temp_1)\n",
    "    feature_7_chamfer_dist_all_1_Test[test_index[mk], 1] = np.std(temp_1)\n",
    "\"\"\"\n",
    "  feature 7, Wasserstein distance\n",
    "\"\"\"\n",
    "feature_7_Wasserstein_dist_all_0 = np.zeros((190, 2))\n",
    "feature_7_Wasserstein_dist_all_1 = np.zeros((190, 2))\n",
    "feature_7_Wasserstein_dist_all_0_Test = np.zeros((np.shape(test_data_Test)[0], 2))\n",
    "feature_7_Wasserstein_dist_all_1_Test = np.zeros((np.shape(test_data_Test)[0], 2))\n",
    "# train\n",
    "for mk in range(190):\n",
    "    temp_0 = []\n",
    "    temp_1 = []\n",
    "    for jk in range(190):\n",
    "        if jk == mk:\n",
    "            continue\n",
    "        else:\n",
    "            if jk in index_train_0:\n",
    "                temp_0.append(Wasserstein_dist_matrix[train_index[jk], train_index[mk]])\n",
    "            elif jk in index_train_1:\n",
    "                temp_1.append(Wasserstein_dist_matrix[train_index[jk], train_index[mk]])\n",
    "    feature_7_Wasserstein_dist_all_0[train_index[mk], 0] = sum(temp_0) / (np.shape(temp_0)[0])\n",
    "    feature_7_Wasserstein_dist_all_1[train_index[mk], 0] = sum(temp_1) / (np.shape(temp_1)[0])\n",
    "    feature_7_Wasserstein_dist_all_0[train_index[mk], 1] = np.std(temp_0) - np.std(temp_1)\n",
    "    feature_7_Wasserstein_dist_all_1[train_index[mk], 1] = np.std(temp_1)\n",
    "# test\n",
    "for mk in range(np.shape(test_data_Test)[0]):\n",
    "    temp_0 = []\n",
    "    temp_1 = []\n",
    "    for jk in range(190):\n",
    "        if jk in index_train_0:\n",
    "            temp_0.append(Wasserstein_dist_matrix_Test[train_index[jk], test_index[mk]])\n",
    "        elif jk in index_train_1:\n",
    "            temp_1.append(Wasserstein_dist_matrix_Test[train_index[jk], test_index[mk]])\n",
    "    feature_7_Wasserstein_dist_all_0_Test[test_index[mk], 0] = sum(temp_0) / (np.shape(temp_0)[0])\n",
    "    feature_7_Wasserstein_dist_all_1_Test[test_index[mk], 0] = sum(temp_1) / (np.shape(temp_1)[0])\n",
    "    feature_7_Wasserstein_dist_all_0_Test[test_index[mk], 1] = np.std(temp_0) - np.std(temp_1)\n",
    "    feature_7_Wasserstein_dist_all_1_Test[test_index[mk], 1] = np.std(temp_1)\n",
    "\n",
    "# combine all features\n",
    "feature_all[:, 23:25] = feature_7_hausdorff_dist_all_0[:, 0:2]\n",
    "feature_all[:, 25:27] = feature_7_hausdorff_dist_all_1[:, 0:2]\n",
    "feature_all[:, 27:29] = feature_7_chamfer_dist_all_0[:, 0:2]\n",
    "feature_all[:, 29:31] = feature_7_chamfer_dist_all_1[:, 0:2]\n",
    "feature_all[:, 31:33] = feature_7_Wasserstein_dist_all_0[:, 0:2]\n",
    "feature_all[:, 33:35] = feature_7_Wasserstein_dist_all_1[:, 0:2]\n",
    "\n",
    "# combine all features for test dataset\n",
    "feature_all_Test[:, 23:25] = feature_7_hausdorff_dist_all_0_Test[:, 0:2]\n",
    "feature_all_Test[:, 25:27] = feature_7_hausdorff_dist_all_1_Test[:, 0:2]\n",
    "feature_all_Test[:, 27:29] = feature_7_chamfer_dist_all_0_Test[:, 0:2]\n",
    "feature_all_Test[:, 29:31] = feature_7_chamfer_dist_all_1_Test[:, 0:2]\n",
    "feature_all_Test[:, 31:33] = feature_7_Wasserstein_dist_all_0_Test[:, 0:2]\n",
    "feature_all_Test[:, 33:35] = feature_7_Wasserstein_dist_all_1_Test[:, 0:2]\n",
    "\n",
    "# normalize all the data\n",
    "for i in range(23, 35):\n",
    "    feature_all[:, i] = preprocessing.normalize([feature_all[:, i]])\n",
    "# print(feature_all[:, 23:35])\n",
    "# print(feature_all[:, 0:2])\n",
    "\n",
    "# normalize all the data\n",
    "for i in range(23, 35):\n",
    "    feature_all_Test[:, i] = preprocessing.normalize([feature_all_Test[:, i]])\n",
    "\n",
    "# feature_all[np.array(train_index)[:,np.newaxis], np.array(list([1, 7,  8, 11, 22, 28, 29, 32]))[:]]\n",
    "# feature_all[np.array(train_index)[:,np.newaxis], np.array(list(range(35)))[:]]\n",
    "# feature_all[np.array(train_index)[:,np.newaxis], np.array(list([7]))[:]]\n",
    "# feature_all[np.array(train_index)[:,np.newaxis], np.array(list(range(0, 16)) + list(range(23, 35)))[:]]\n",
    "# feature_all[np.array(train_index)[:,np.newaxis], np.array(list(range(0, 23)))[:]]\n",
    "X_train = feature_all[np.array(train_index)[:, np.newaxis], np.array(list(range(35)))[:]]\n",
    "y_train = all_label[train_index]\n",
    "X_test = feature_all_Test[np.array(test_index)[:, np.newaxis], np.array(list(range(35)))[:]]\n",
    "y_test = all_label_Test[test_index]\n",
    "\n",
    "# balancing the class classification for the training data\n",
    "sm = SMOTE(random_state=i, k_neighbors=1)  # set the seed of smote as the index of the loop\n",
    "X_train, y_train = sm.fit_resample(X_train, y_train)\n",
    "index_train_0_try = np.where(y_train == 0)[0]\n",
    "index_train_1_try = np.where(y_train == 1)[0]\n",
    "#   print(index_train_0_try)\n",
    "#   print(index_train_1_try)\n",
    "\n",
    "# print(np.shape(X_train))\n",
    "# print(np.shape(y_train))\n",
    "\n",
    "# # PCA\n",
    "# pca = PCA(n_components = 10)\n",
    "# X_train = pca.fit_transform(X_train)\n",
    "# X_test = pca.transform(X_test)\n",
    "\n",
    "# model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "model = XGBClassifier(\n",
    "    learning_rate=0.01,\n",
    "    n_estimators=5000,\n",
    "    max_depth=9,\n",
    "    min_child_weight=1,\n",
    "    gamma=0,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=1,\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='error@0.001',\n",
    "    nthread=4,\n",
    "    scale_pos_weight=1,\n",
    "    seed=27)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# predict the results\n",
    "y_pred = model.predict(X_test)\n",
    "# y_pred = np.logical_not(y_pred).astype('int64')  # be careful, we flip 0 and 1 here\n",
    "# y_test = np.logical_not(y_test).astype('int64')  # be careful, we flip 0 and 1 here\n",
    "# print(classification_report(y_test,y_pred))\n",
    "mean_accuracy.append(accuracy_score(y_test, y_pred))\n",
    "mean_precision.append(precision_score(y_test, y_pred))\n",
    "mean_recall.append(recall_score(y_test, y_pred))\n",
    "mean_F1.append(f1_score(y_test, y_pred))\n",
    "# print('accuracy:', accuracy_score(y_test, y_pred))\n",
    "print('The confusion matrix is:')\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# print(y_pred)\n",
    "# print(y_test)\n",
    "N = 1\n",
    "print('The accuracy is:', sum(mean_accuracy)/N)\n",
    "print('The precision is:', sum(mean_precision)/N)\n",
    "print('The recall is:', sum(mean_recall)/N)\n",
    "print('The F1-score is:', sum(mean_F1)/N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "cENdnG7dBK5q",
    "mR6HwS49Bdhy",
    "bXUQJOxa_wSd",
    "frKHdEmyaWRA",
    "jfNBR0gmRWIR",
    "5rZoQhs1TZfP",
    "Sv4j8p8wEBeC",
    "8YWAJ2bHBfee",
    "6gUQxpggOWG_",
    "kxtSkTJ1KPYF"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
